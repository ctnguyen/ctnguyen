%Texlive-full Version 3.141592-1.40.3 (Web2C 7.5.6)
%Kile Version 2.0.83

\documentclass[a4paper,10pt]{article}
\usepackage[utf8x]{inputenc}

\usepackage{lmodern}
\usepackage[a4paper]{geometry}

\usepackage{hyperref}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{pstricks}
\usepackage{pst-node}


\begin{document}
%%%%%%%%%%%%%%%%%% DOCUMENT TITLE %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}Memento for Usuals Probability Distributions\end{center}
%%%%%%%%%%%%%%%%%% DOCUMENT TITLE %%%%%%%%%%%%%%%%%%%%%%%%%
\section{Base }
\paragraph{Convergences and relations}
\begin{center}
\begin{pspicture}(-7,-2)(7,2)
%\psline(-7,-2)(7,2)
\rput(-1, 1){\Rnode{as}{\psframebox{$X_n \xrightarrow{a.s} X$}}}
\rput(-1,-1){\Rnode{Lp}{\psframebox{$X_n \xrightarrow{\textbf{L}^p} X$}}}
\rput(-4,-1){\Rnode{Lq}{\psframebox{$X_n \xrightarrow{\textbf{L}^q} X$}}}
\rput( 2, 0){\Rnode{P}{\psframebox{$X_n \xrightarrow{\mathbb{P}} X$}}}
\rput( 5, 0){\Rnode{L}{\psframebox{$X_n \rightsquigarrow X$}}}
\rput( -5, 0.5){\Rnode{cond}{$ _{0<p<q \leq \infty} \Rightarrow \begin{array}{l}
                                                                     |.|_p \leq |.|_q \\
								    \textbf{L}^q \subset \textbf{L}^p
                                                                    \end{array}$}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ncline[linestyle=dashed]{->}{Lq}{Lp}
\ncline[linestyle=dashed]{->}{as}{P}
\ncline[linestyle=dashed]{->}{Lp}{P}
\ncline[linestyle=dashed]{->}{P}{L}
\end{pspicture}
\end{center}


\paragraph{Law of large numbers}
\[
\text{if} \hspace{5mm} (X_{n})_{(n\geqslant 0)} \in \textbf{L}^1 \text{  i.i.d,} \hspace{5mm} \text{ then  } \hspace{5mm}  \overline{X}_n\xrightarrow{a.s} \mathbb{E}[X_{0}]  
\]
\paragraph{Central Limit Theorem}
\[
\text{if} \hspace{5mm} (X_{n})_{(n\geqslant 0)} \in \textbf{L}^2 \text{  i.i.d,} \hspace{5mm} \text{ then  } \hspace{5mm}
\frac{\overline{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} \rightsquigarrow   \mathcal{N}(0,1)
 \hspace{5mm}  \text{    or} \hspace{5mm}  
\sqrt{n}(\overline{X}_n - \mu) \rightsquigarrow   \mathcal{N}(0,\sigma^{2})
\]

\paragraph{Slutsky's Theorem}
\[
\text{if} \hspace{5mm} 
\left\{ 
\begin{array}{l}
X_n \rightsquigarrow X \\
Y_n \xrightarrow{\mathbb{P}} c 
\end{array}\right. 
\hspace{5mm} \text{ then  } \hspace{5mm}
\left\{ 
\begin{array}{l}
X_n + Y_n \rightsquigarrow X+c \\
X_n . Y_n \rightsquigarrow  c.X 
\end{array}\right. 
\]

\[
\text{if} \hspace{5mm} 
\left\{ 
\begin{array}{l}
a_n \rightsquigarrow +\infty \\
a_n(X_n - c) \rightsquigarrow X \\
\end{array}\right. 
\hspace{5mm} \text{ then  } \hspace{5mm} X_n \xrightarrow{\mathbb{P}} c
\]


\paragraph{Markov's inequality}
\[
\text{for all}\hspace{5mm} \varepsilon > 0, p>0, X\in \textbf{L}^p \hspace{20mm} \mathbb{P}(|X| \geq \varepsilon) \leq \frac{\mathbb{E}[|X|^p]}{\varepsilon ^p}
\]
\paragraph{Bienayme-Tchebychev's inequality}
\[
\text{for all  } \hspace{5mm} \varepsilon > 0, X\in \textbf{L}^2 \hspace{20mm} \mathbb{P}(|X- \mathbb{E}X| \geq \varepsilon) \leq \frac{\textbf{Var}[X]}{\varepsilon ^2}
\]
\paragraph{Berry-Esseen's inequality}
\[
\text{if} \hspace{5mm} (X_{i})_{(0\leqslant i \leqslant n)} \in \textbf{L}^3 \text{  i.i.d,} \hspace{5mm} \text{ then  } \hspace{5mm}
\sup_{t\in \mathbb{R}} \left| \mathbb{P}(\frac{\overline{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} \leq t) - F_{\mathcal{N}_{0,1}}(t) \right| 
\leq \frac{\mathbb{E}[|X_i - \mu|^3]}{\sigma^3 \sqrt{n}}
\]
\paragraph{Hoeffding's inequality}
if $(X_{i})_{(0\leqslant i \leqslant n)}$ independant bounded $X_i \in [a_i,b_i]$, then
\[
\mathbb{P}(\overline{X}_n- \mathbb{E}[\overline{X}_n] \geq t) \leq exp\left(-\frac{2n^2t^2}{\sum_{i=0}^{n} (b_i-a_i)^2}\right) 
\hspace{5mm} \text{ for all } t >0
\]

\paragraph{Dvoretzky–Kiefer–Wolfowitz (Massart) inequality}
if $(X_{i})_{(0\leqslant i \leqslant n)}$ i.i.d  $X_i \sim F $, and its empirical distribution $F_n$ 
\[
\left\{
\begin{array}{l}
\mathbb{P}( \sup_{x\in \mathbb{R}} ( {F}_n(x)- F(x) ) > \varepsilon ) \leq e^{-2n\varepsilon^2} 
\hspace{5mm} \text{ for all } \varepsilon \geq \sqrt{\frac{1}{2n}log2} \\ \\
\mathbb{P}( \sup_{x\in \mathbb{R}} | {F}_n(x)- F(x) | > \varepsilon ) \leq 2.e^{-2n\varepsilon^2} 
\hspace{5mm} \text{ for all } \varepsilon > 0
\end{array}
\right.
\]


\paragraph{Kolmogorov's inequality}
if $(X_{i})_{(0\leqslant i \leqslant n)} \in \textbf{L}^2$ independant, $\mathbb{E}[X_i]=0$, then
\[
\mathbb{P}\left( \max_{0\leqslant k \leqslant n}\left| \sum_{i=0}^{k} X_i  \right| 
\geq \varepsilon \right) \leq \frac{1}{\varepsilon^2} \sum_{i=0}^{n} \textbf{Var}[X_i]
\hspace{5mm} \text{ for all } \varepsilon >0
\]

\paragraph{Gamma Function}
\[
\Gamma(z) = \int_{0}^{\infty} t^{z-1}e^{-t}dt 
\hspace{30mm}  \Gamma(1) = 1 
\hspace{10mm}  \Gamma(z+1) = z\Gamma(z) 
\hspace{10mm}  \Gamma(n) = (n-1)! 
\]
\paragraph{Beta Function}
\[
\beta(x,y)=  \int_{0}^{1}  t^{x-1}(1-t)^{y-1}dt  \hspace{30mm} \beta(y,x)=\beta(x,y)= \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}
\]
\paragraph{Binomial Coefficient k-combination of n elements}
\[
\textbf{C}^{k}_{n} = \binom{n}{k} = \frac{n!}{k!(n-k)!}
\hspace{30mm} \textbf{C}^{k}_{n} = \textbf{C}^{n-k}_{n} 
\hspace{10mm} \textbf{C}^{k+1}_{n+1} = \textbf{C}^{k}_{n} + \textbf{C}^{k+1}_{n} 
\]

\paragraph{Generating Function (discrete variables)}
\[
\textbf{G}_{X}(t)= \mathbb{E}[t^{\textbf{X}}] = \sum t^{x}.p(x)
\hspace{6mm} \mathbb{P}(\textbf{X}=k) = \frac{\textbf{G}_{X}^{(k)}(0)}{k!}
\hspace{6mm} \textbf{G}_{X}^{(m)}(1)  = \mathbb{E}[(X)(X-1)...(X-m+1)]
\]
\paragraph{Characteristic Function}
\[
\mathbb{R}^{n}
\left\{
\begin{array}{l}
\varphi_{X}(t)= \mathbb{E}[e^{i<t,X>}] = \int_{\mathbb{R}} e^{i<t,x>} . f_{X}(x).dx \\
f_{X}(x) = \frac{1}{2\pi}\int_{\mathbb{R}} e^{i<t,x>} . \varphi_{X}(t).dt
\end{array}\right. 
\hspace{5mm}
(\mathbb{E}[X^{k}]< \infty) \Rightarrow 
\left\{
\begin{array}{l}
\varphi_{X}^{(r)}(t)= i^{r}\mathbb{E}[X^{r}e^{itX}] \\
\varphi_{X}^{(r)}(0)= i^{r}\mathbb{E}[X^{r}]
\end{array}\right. 
\]


\section{Discrete Probability Distributions}

\paragraph{Bernouilli} $X\sim \mathcal{B}(\theta)$ when, 
\[
X \in \{0,1\} , 
\hspace{10mm}
\left\{
\begin{array}{l}
P(X=1)=\theta \\
P(X=0)=1-\theta
\end{array}\right.
\hspace{20mm}
\mathbb{E}[X^k]=\theta,
\hspace{10mm}
\left\{
\begin{array}{l}
\mathbb{E}[X]=\theta \\
\textbf{Var}[X]=\theta (1-\theta)
\end{array}\right.
\]

\paragraph{Binomial} $X\sim \mathcal{B}(n,\theta)$ when,
\[
X \in \{0,1 .. , n\} , 
\hspace{10mm}
P(X=k)=C^k_n\theta ^k (1-\theta)^{n-k} 
\hspace{10mm}
\left\{
\begin{array}{l}
\mathbb{E}[X]=n\theta \\
\textbf{Var}[X]=n \theta (1-\theta) 
\end{array}\right.
\]
Binomial variable is the sum i.i.d of n Bernouilli variable : $\sum_1^n \mathcal{B}(\theta) \stackrel{\textbf{i.i.d}}{\sim}  \mathcal{B}(n,\theta) $

\paragraph{Poisson} $X\sim \mathcal{P}(\lambda)_{\lambda>0}$ when,
\[
X \in \mathbb{N} , 
\hspace{10mm}
P(X=k)= e^{-\lambda}\frac{\lambda^k}{k!}
\hspace{10mm}
\left\{
\begin{array}{l}
\mathbb{E}[X]=\lambda \\
\textbf{Var}[X]=\lambda
\end{array}\right.
\]
The sum of independant \textbf{Poisson} variable is a \textbf{Poisson} variable of sum :
\[
\text{if  } 
\left\{
\begin{array}{l}
X_i \sim \mathcal{P}(\lambda_i) \textbf{independant} \\
S_n = \sum_{1}^{n} X_i \\
\lambda = \sum_{1}^{n} \lambda_i
\end{array}\right.
\hspace{10mm} \text{then} \hspace{10mm}
S_n \sim \mathcal{P}(\lambda)
\]
When $n\theta \approx \lambda$ we have $\mathcal{B}(n,\theta) \approx \mathcal{P}(\lambda)$

\paragraph{Uniform} $X\sim \mathcal{U}_{[a,b]}$ when $X \in [a,b]$ and,
\[
f_{X}(x) = \frac{1}{b-a}\mathbf{1}_{[a,b]}
\hspace{10mm}
\left\{
\begin{array}{l}
\mathbb{E}[X^k]=\frac{b^{k+1}-a^{k+1}}{(k+1)(b-a)} \\
\mathbb{E}[X]=\frac{a+b}{2} \\
\textbf{Var}[X]=\frac{(b-a)^2}{12} 
\end{array}\right.
\hspace{10mm}
\left\{
\begin{array}{l}
\textbf{F}_X(x) = \frac{x-a}{b-a}\mathbf{1}_{[a,b[} + \mathbf{1}_{[b,\infty[} \\
\varphi_{X}(t) = \frac{e^{itb}-e^{ita}}{it(b-a)}
\end{array}\right.
\]
\[
-\text{log}(\mathcal{U}_{[0,1]}) = \mathcal{E}(1)
\]

\paragraph{Exponential} $X\sim \mathcal{E}(\lambda)_{\lambda>0}$ when $X \in [0,\infty[$ and,
\[
f_{X}(x) = \lambda e^{-\lambda x} \mathbf{1}_{[0,\infty[}
\hspace{10mm}
\left\{
\begin{array}{l}
\mathbb{E}[X^k]=\frac{k!}{\lambda^k} \\
\mathbb{E}[X]=\frac{1}{\lambda} \\
\textbf{Var}[X]=\frac{1}{\lambda^2} 
\end{array}\right.
\hspace{10mm}
\left\{
\begin{array}{l}
\textbf{F}_X(x) = (1- e^{-\lambda x})\mathbf{1}_{[0,\infty[} \\
\varphi_{X}(t) = \frac{\lambda}{\lambda - it}
\end{array}\right.
\]
Some relations
\[
aX \sim \mathcal{E}(\frac{\lambda}{a})
\hspace{10mm}
\mathcal{E}(\lambda) = \gamma(1,\lambda)
\hspace{10mm}
\overline{X}_n \sim \gamma(n,n\lambda)
\hspace{10mm}
X_{(1)} \sim \mathcal{E}(n\lambda)
\]

\paragraph{Gamma} $X\sim \gamma(a,b)_{a>0,b>0}$ when $X \in [0,\infty[$ and,
\[
f_{X}(x) = \frac{b^a}{\Gamma(a)} x^{a-1} e^{-bx} \mathbf{1}_{[0,\infty[}
\hspace{10mm}
\left\{
\begin{array}{l}
\mathbb{E}[X^k]=\frac{\Gamma(a+k)}{b^k\Gamma(a)} \\
\mathbb{E}[X]=\frac{a}{b} \\
\textbf{Var}[X]=\frac{a}{b^2} 
\end{array}\right.
\hspace{10mm}
\left\{
\begin{array}{l}
\textbf{F}_X(x) = \frac{\gamma(a,bx)}{\Gamma(a)} (*)  \\
\varphi_{X}(t) = (\frac{b}{b - it})^a
\end{array}\right.
\]
\[
\varepsilon X \sim \gamma(a,\frac{b}{\varepsilon})
\hspace{20mm}
X_i \stackrel{\textbf{i.i.d}}{\sim} \gamma(a_i,b) \Longrightarrow \sum X_i \sim \gamma(\sum a_i, b)
\hspace{20mm}
\mathbb{E}[\frac{1}{X}] \text{ , } \mathbb{E}[\frac{1}{X^2}]
\]
(*) The imcomplete gamma function is defined by $\gamma(a,x) = \int_{0}^{x} t^{a-1}e^{-t}dt $

\paragraph{Normal} $X \sim \mathcal{N}(\mu,\sigma^2)$ when $X \in \mathbb{R}$ and,
\[
f_{X}(x) = \frac{1}{\sigma \sqrt{2\pi}} exp (-\frac{(x-\mu)^2}{2\sigma^2})
\hspace{10mm}
\left\{
\begin{array}{l}
\mathbb{E}[X]=\mu \\
\textbf{Var}[X]=\sigma^2
\end{array}\right.
\hspace{10mm}
\left\{
\begin{array}{l}
\textbf{F}_X(x) =  \frac{1}{2}(1+erf(\frac{x}{\sqrt{2}})) \\
\varphi_{X}(t) = exp(i\mu t - \frac{1}{2}\sigma^2 t^2)
\end{array}\right.
\]
\[
\frac{X - \mu}{\sigma} \sim \mathcal{N}(0,1)
\hspace{10mm}
f_{X}(x) = \frac{1}{\sigma}f_{\mathcal{N}(0,1)}(\frac{x-\mu}{\sigma})
\hspace{10mm}
\left\{
\begin{array}{l}
F^{\mathcal{N}(0,1)} (\alpha) + F^{\mathcal{N}(0,1)}(-\alpha) = 1 \\
q^{\mathcal{N}(0,1)}_{1-\alpha} = - q^{\mathcal{N}(0,1)}_{\alpha} 
\end{array}\right.
\]

\paragraph{Chi-squared} $X\sim \chi^2_n= \gamma(\frac{n}{2}, \frac{1}{2})$ when  $X \in \mathbb{R}_+$ and
\[
X = \sum^{n}_{\textbf{i.i.d}} [ \mathcal{N}(0,1) ]^2
\hspace{20mm}
\left\{
\begin{array}{l}
\mathbb{E}[X]= n \\
\textbf{Var}[X]=2n
\end{array}\right.
\]


\paragraph{Student} $T\sim \textbf{t}_m$ when  $T \in \mathbb{R}$ and
\[
\left[
\begin{array}{l}
Y \bot Z                \\
Y \sim \mathcal{N}(0,1) \\
Z \sim  \chi^2_m  
\end{array}\right.
\hspace{3mm} \Rightarrow
T=\frac{Y}{\sqrt{\frac{Z}{m}}} \sim \textbf{t}_n
\hspace{10mm}
\textbf{Var}[T]= \frac{m}{m-2} , \text{  when } m>2 
\]


\paragraph{Fisher} $F\sim \mathcal{F}_{(m_1,m_2)}$ when  $F \in \mathbb{R}_+$ and
\[
\left[
\begin{array}{l}
X_1 \bot X_2            \\
X_1 \sim \chi^2_{(m_1)} \\
X_2 \sim \chi^2_{(m_2)}
\end{array}\right.
\hspace{3mm} \Rightarrow
F=\frac{\frac{X_1}{m_1}}{\frac{X_2}{m_2}} \sim \mathcal{F}_{(m_1,m_2)}
\hspace{10mm}
F\sim \mathcal{F}_{(m_1,m_2)} \Rightarrow \frac{1}{F} \sim \mathcal{F}_{(m_2,m_1)} 
\]



\section{Gaussian}
\paragraph{Cochran's theorem}

\paragraph{Gaussian samples}
\[
\text{if }
\left\{
\begin{array}{l}
X_i \stackrel{\textbf{i.i.d}}{\sim} \mathcal{N}(\mu, \sigma^2) \\
\overline{X}_n = \frac{1}{n}\sum^n_1 X_i \\
S^2 = \frac{1}{n-1}\sum^n_1 (X_i - \overline{X}_n) 
\end{array}\right.
\hspace{10mm}
\text{then }
\left\{
\begin{array}{l}
\frac{\sqrt{n} (\overline{X}_n - \mu)}{\sigma} \sim \mathcal{N}(0,1) \\
\frac{1}{\sigma2}\sum^n_1 (X_i - \mu)^2 \sim \chi^2_{(n)} \\
\frac{1}{\sigma2}\sum^n_1 (X_i - \overline{X}_n)^2 \sim \chi^2_{(n-1)} \\
\frac{\sqrt{n} (\overline{X}_n - \mu)}{S} \sim \textbf{t}_{(n-1)}
\end{array}\right.
\]

\section{General formula}
\paragraph{Standard normal distribution :} 
\[
\left\{
\begin{array}{rcl}
-q_{\alpha} &=& q_{1-\alpha}   \\
\phi(-t)    &=& 1 - \phi(t)
\end{array}\right.
\]
\paragraph{Translated and Scaled variables}
\[
X=\sigma Y+\mu 
\Longleftrightarrow
Y=\frac{X-\mu}{\sigma}
\Longleftrightarrow
f_{X}(x) = \frac{1}{|\sigma|} f_{Y}(\frac{x-\mu}{\sigma}) 
\]
\paragraph{Translated variables}
\[
X= Y+\mu 
\Longleftrightarrow
Y=X-\mu
\Longleftrightarrow
f_{X}(x) = f_{Y}(x-\mu) 
\]
\paragraph{Tests multi-parameters level $\alpha$} 
\begin{enumerate}
 \item build the confidence region $\textbf{D}_{1-\alpha}$ of level $1-\alpha$ 
 \begin{itemize}
 \item $\textbf{for unilateral test}$ $\textbf{D}_{1-\alpha}$ is a unilateral interval opposing sens of $\Theta_0$ 
 \item $\textbf{for bilateral test}$ $\textbf{D}_{1-\alpha}$ is a bilateral interval
 \end{itemize}
 \item Hypothesis $H_0$ is rejected when $\textbf{D}_{1-\alpha} \cap \Theta_0 = \varnothing$
\end{enumerate}

\paragraph{The Delta Method}
\[
\text{if} \hspace{5mm} 
\left\{ 
\begin{array}{l}
\sqrt{n}(U_n - \mu) \rightsquigarrow   \mathcal{N}(0,\sigma^{2}) \\
g \in \mathbb{C}^1_{(\mu)}
\end{array}\right. 
\hspace{5mm} \text{ then  } \hspace{5mm}
\sqrt{n}(g(U_n) - g(\mu) ) \rightsquigarrow   \mathcal{N}(0,\sigma^{2}[g'(\mu)]^2) 
\]
\paragraph{Variance stabilizing transformations}
\[
\widehat{\theta}_n \xrightarrow{P} \theta
\hspace{2cm}
\sqrt{n}(\widehat{\theta}_n - \theta) \rightsquigarrow   \mathcal{N}(0,\sigma^{2}(\theta))
\]
This situation is difficult because the asymtotic law on the right hand depend to $\theta$ parameter. There are 3 methods :
\begin{itemize}
 \item if $\sigma(\theta)=a\theta$ then $\sqrt{n}(\frac{\widehat{\theta}_n}{a\theta} - \frac{1}{a}) \rightsquigarrow   \mathcal{N}(0,1)$
 \item if $\sigma(\theta)$ is continuous then $\sigma(\widehat{\theta}_n) \xrightarrow{P} \sigma(\theta)$ and by Slutsky
$\sqrt{n}\frac{\widehat{\theta}_n - \theta}{\sigma(\widehat{\theta}_n)} \rightsquigarrow   \mathcal{N}(0,1)$
 \item or we find $\phi(\theta)$ such $\sqrt{n}[\phi(\widehat{\theta}_n) - \phi(\theta)] \rightsquigarrow   \mathcal{N}(0,[\sigma(\theta).\phi'(\theta)]^2)$
by resolving $\phi'(\theta) = \frac{1}{\sigma(\theta)}$
\end{itemize}
\paragraph{sufficient statistics}
\[
\mathbb{E}_{\theta}[X|T(X)] = \mathbb{E}[X|T(X)]  \Longleftrightarrow
\mathbb{P}_{\theta}[X|T(X)] = \mathbb{P}[X|T(X)]  \Longleftrightarrow 
f(\theta, x) = h(x).g(\theta, T(x))
\]
We can define a order relation of sufficient statistics and then a notion of minimal sufficient statistics
\[
S(X) \preccurlyeq T(X) \text{ if } \exists \varphi \text{ that } S(X) = \varphi(T(X))
\]


\section{Linear Regression formula}
The general linear model is described by equation :
\[
\textbf{Y} = \textbf{X} \beta + \varepsilon    
\hspace{1.5cm} 
\left\{
\begin{array}{l}
 \text{dim}(\textbf{X})=[n,p] , p\leq n \\
 \varepsilon \text{  i.i.d of variance }\sigma^2
\end{array}\right. 
\]
\paragraph{General case} If the identifiability condition is satisfied ($\textbf{X}^t \textbf{X}$ invertible or $\textbf{X}$ is full rank), then the estimator existe and has propreties
\begin{itemize}
 \item $\widehat{\beta} = (\textbf{X}^t \textbf{X})^{-1} \textbf{X}^t \textbf{Y} $
 \item $\mathbb{E}_{\beta}[\widehat{\beta}] = \beta $
 \item $\textbf{Cov}_{\beta}[\widehat{\beta}] = \sigma^2 (\textbf{X}^t \textbf{X})^{-1} $
 \item $\mathbb{E}_{\beta}[(\widehat{\beta}_i - \beta_i)^2 ] = \textbf{Var}_{\beta}[\widehat{\beta}_i] = \sigma^2 [(\textbf{X}^t \textbf{X})^{-1}]_{(i,i)} \hspace{1cm}$ for i-th parameter
 \item $\widehat{s}^2_n = \frac{\|\widehat{\varepsilon}\|^2}{n-p} \hspace{1cm}$ where $\widehat{\varepsilon} = \textbf{Y} - \textbf{X}\widehat{\beta} $ is an unbiased estimator of $\sigma^2$
\end{itemize}
\paragraph{Gaussian case} when the gaussian vector $\varepsilon \sim \mathcal{N}(0,\sigma^2.\textbf{I}_n)$, everything is gaussian, then the model has several propreties
\begin{itemize}
 \item $\widehat{\beta} \sim \mathcal{N}(\beta,\sigma^2.(\textbf{X}^t \textbf{X})^{-1}) $
 \item $\frac{n-p}{\sigma^2} \widehat{s}^2_n = \frac{\|\widehat{\varepsilon}\|^2}{\sigma^2} \sim \chi^2_{n-p}  $
 \item $\widehat{\beta} \bot \widehat{s}^2_n $
\end{itemize}

\section{Empirical estimation formules}
\paragraph{Empirical estimator : }$\hspace{1cm} \overline{X}_n\xrightarrow{P} \mathbb{E}[X_{1}] \hspace{1cm} \sqrt{n}(\overline{X}_n -\mathbb{E}[X_{1}]) \rightsquigarrow   \mathcal{N}(0,\sigma^2) $

\paragraph{Empirical distribution : }
\[
\nu_n(\omega) = \frac{1}{n}\sum^{n}_{1} \delta_{X_i}(\omega) \hspace{2cm} 
\mathbb{F}_n(x) = \frac{1}{n} \sum^{n}_{1} \mathbf{1}_{]-\infty;x]}(X_i) 
\]
There are several propreties for empirical distribution :
\begin{itemize}
 \item $n\mathbb{F}_n(x) \sim \mathcal{B}(n,\mathbb{F}(x) )$
 \item $\mathbb{F}_n(x) \xrightarrow{a.s}   \mathbb{F}(x)$
 \item $\sqrt{n}(\mathbb{F}_n(x) -  \mathbb{F}(x)) \rightsquigarrow   \mathcal{N}(0,\mathbb{F}(x)[1-\mathbb{F}(x)] )$
 \item $\sup_{x} \| \mathbb{F}_n(x) -  \mathbb{F}(x) \| \xrightarrow{a.s} 0 $  (Glivenko-Centelli)
\end{itemize}
\paragraph{Empirical Quantile : }
\[
\left\{
\begin{array}{l}
\mathbb{F}^{-1}(p) = inf \{ x \in \mathbb{R} , \mathbb{F}(x) \geq p \} \hspace{1cm} \text{ then } \\
x_p(n) = \mathbb{F}_n^{-1}(p)
\end{array}\right.
\hspace{1cm}
\left\{
\begin{array}{l}
 \mathbb{F}(x) \geq p \Longleftrightarrow x \geq \mathbb{F}^{-1}(p) \\
 \mathbb{F}^{-1}\mathbb{F}(x) \leq x   \\
 \mathbb{F}\mathbb{F}^{-1}(p) \geq p   
\end{array}\right. 
\]
Since the empirical distribution cumulative function is a Bernouilli, the empirical quantile is defined by order statistics
\[
x_p(n) = X_{(i)} \hspace{1cm} \text{if} \hspace{1cm} \frac{i-1}{n} < p \leq \frac{i}{n}
\]
If the cumulative function is smooth at point $x_p$ , i.e differentiable and $f(x_p)>0$ (uniqueness), then the empirical quantile converge and asymtotically normal :
\[
x_p(n) \xrightarrow{P} x_p 
\hspace{2cm} \text{and} \hspace{2cm} 
\sqrt{n}(x_p(n) - x_p ) \rightsquigarrow   \mathcal{N}(0,\frac{p(1-p)}{f^2(x_p)} )
\]
\paragraph{Confidence band level $1-\alpha$: }
By using the $\textbf{Massart inequality}$
\[
\mathbb{P}( \sup_{x} | \mathbb{F}_n(x) -  \mathbb{F}(x) | > \frac{t}{\sqrt{n}} ) \leq 2.e^{-2t^2}
\]
and choose 
\[
\alpha =  2.e^{-2t^2} \hspace{20mm} \text{equivalent} \hspace{20mm} t_{\alpha} = \sqrt{ -\frac{\text{log}(\frac{\alpha}{2})}{2n} }
\]

\section{Exponential model formules}
From the analyse result
\[
J(\eta)=\int e^{\eta T(x)} d\nu(x) 
\hspace{10mm} \text{is infinitly differentiable } \hspace{10mm}
J^{(i)}(\eta)=\int [T(x)]^i e^{\eta T(x)} d\nu(x) 
\]
Where $H$ the definition domain of $J(\eta)$ is a convex and the differentielle is valid on its interior
\paragraph{Canonical model}
\[
g_{\eta}(x) = exp [\eta T(x) - A(\eta)]
\hspace{10mm} \text{with} \hspace{10mm}
 A(\eta) = log (\int e^{ \eta T(x)}  d\nu(x) ) 
\]
\[
 A'(\eta) = \mathbb{E}_{\eta}[T(X)]  
\hspace{30mm} 
 A''(\eta)= \textbf{Var}_{\eta}[T(X)]  > 0
\]
is identifiable and well defined when 
\begin{itemize}
 \item ${\eta \in H}$ is a convex of non-empty interior
 \item $T(x)$ is non-constant $\nu$-a.s
\end{itemize}
Its log-likelihood function is 
\[
L_{\eta}(X_1,X_2,...X_n) = n[\eta \overline{T}_n -A(\eta)]
\]
And its sufficient statistics converge
\[
\overline{T}_n \xrightarrow{a.s} A'(\eta) 
\hspace{20mm}
\sqrt{n}(\overline{T}_n - A'(\eta) ) \rightsquigarrow   \mathcal{N}(0,A''(\eta))
\]
By inverting the $A'(\eta)$ with $\delta$-method, we define the convergent maximun likelihood estimator 
\[
\widehat{\eta}_n \xrightarrow{a.s} \eta 
\hspace{20mm}
\sqrt{n}(\widehat{\eta}_n - \eta ) \rightsquigarrow   \mathcal{N}(0,\frac{1}{A''(\eta)} ) = \mathcal{N}(0,\frac{1}{\textbf{Var}_{\eta}(T)} )
\]

\paragraph{General model}
The general model issu from the variable transform and mesure transform of the canonical model
\[
d\nu = h.d\mu  \hspace{10mm} \eta = Q(\theta) \hspace{10mm}  \theta = Q^{-1}(\eta) 
\]
Under conditions 
\begin{enumerate}
 \item $\Theta$ is a interval not reduced to a point
 \item $T(x)$ is not constant $\nu$-a.s
 \item $h(x) \geq 0$ is a must, and we suppose $C(\theta) \geq 0$
 \item $Q(\theta)$ is continuous and strictly monotonic (admitting inversion). 
 \item if $Q(\theta) \in C^{1}(\theta)$ and $Q'\neq 0$ the the model is regular
\end{enumerate}

\[
f_{\theta}(x) = C(\theta) exp[Q(\theta) T(x)] h(x)
\hspace{10mm} \text{with} \hspace{10mm}
C(\theta) = (\int exp[Q(\theta) T(x)] h(x) d\mu(x) ) ^{-1}
\]
\[
A(\eta) = -\text{log}C(\theta)
\hspace{20mm} 
\mathbb{E}_{\theta}[\varphi(X)]  = \mathbb{E}_{\eta}[\varphi(X)]
 \hspace{10mm}
\textbf{Var}_{\theta}[\varphi(X)]  = \textbf{Var}_{\eta}[\varphi(X)]
\]
\paragraph{Maximun likelihood estimator}
By inverting the $Q(\eta)$ with $\delta$-method, we define the convergent maximun likelihood estimator 
\[
\widehat{\theta}_n \xrightarrow{a.s} \theta 
\hspace{20mm}
\sqrt{n}(\widehat{\theta}_n - \theta ) \rightsquigarrow   \mathcal{N}(0,\frac{1}{\textbf{Var}_{\theta}(T)[Q'(\theta)]^2  } )
\]
\section{Regular models}
\paragraph{Regularity Definition}  {$\mathbb{P}_{\theta},\theta \in \Theta$}
\begin{enumerate}
 \item the function $\theta \mapsto \xi_{\theta}(x)=\sqrt{f_{\theta}(x)}$ is a.s absolute continuous.
 \item $\forall \theta, \xi'_{\theta}(x) \in L^2(d\mu)$ and the map $\theta \mapsto \xi'_{\theta}(x)$ is continuous in $L^2(d\mu)$
 \item its Fisher information is $I(\theta) = 4 \| \xi'_{\theta} \|^{2}_{L^2_{\mu}}$
\end{enumerate}

\paragraph{Hypothesis 1}
\begin{enumerate}
 \item the support {$x, f_{\theta}(x)>0$} is independent of $\theta$
 \item $f_{\theta}(x) \in C^1_{(\theta)}$ for $\mu$-a.e $x$ points  
 \item The family {$\frac{f'^2_{\theta}}{f_{\theta}} $} are locally dominated in $L^1_{\mu}$ space
 \item its Fisher informqtion is 
\[
I(\theta) 
= \mathbb{E}_{\theta}[( \frac{\partial}{\partial \theta} \text{log}f_{\theta} )^2]
= \textbf{Var}_{\theta} [ \frac{\partial}{\partial \theta} \text{log} f_{\theta}]  
\]
\end{enumerate}

\paragraph{Hypothesis 2}
\begin{enumerate}
 \item $f_{\theta}(x) \in C^2_{(\theta)}$ for $\mu$-a.e $x$ points 
 \item The family {$f''_{\theta}$} are locally dominated in $L^1_{\mu}$ space
 \item its Fisher informqtion is $I(\theta) = -\mathbb{E}_{\theta}[ \frac{\partial^2}{\partial \theta^2} \text{log}f_{\theta} ]$
\end{enumerate}
\paragraph{Translated models}
\[
\text{if }
\left\{
\begin{array}{l}
\xi = \sqrt{f} \text{ is a.c } \\
\xi' \in L^2(dx)
\end{array}\right.
\hspace{10mm}
\text{ then }
f_{\theta}(x)=f(x-\theta) \text{ is regular and } I(\theta)=4\| \xi' \|^2 
\]
\paragraph{Exponential models}
\[
\text{if } Q(\theta) \in C^{1}(\theta) \text{ and } Q'\neq 0 \text{ the exponential model is regular } I(\theta) = \textbf{var}_{\theta}(T)[Q'(\theta)]^2
\]
\paragraph{Parameters change}
\[
\text{if }
\left\{
\begin{array}{l}
f_{\theta}(x) ,I(\theta) , \theta \in \Theta \text{ regular model} \\
\eta = g(\theta), h(\eta)=g^{-1}(\eta) \in C^1(\eta)
\end{array}\right.
\text{ then }
\left\{
\begin{array}{l}
k_{\eta}(x) = f_{g^{-1}(\eta)} ,J(\eta), \eta \in g(\Theta) \text{ is regular} \\
\text{Its Fisher information } J(\eta) =  h'^2(\eta) I(h(\eta)) 
\end{array}\right.
\]
\paragraph{Product models}
\[
\text{if }
\left\{
\begin{array}{l}
f_{x,\theta}, \xi_{x,\theta} , I_{x}(\theta) , \theta \in \Theta \text{ regular} \\
f_{y,\theta}, \xi_{y,\theta} , I_{y}(\theta) , \theta \in \Theta \text{ regular} \\
\text{The two model are independent}
\end{array}\right.
\text{ then }
\left\{
\begin{array}{l}
f_{x,\theta} \bigotimes f_{y,\theta}, \theta \in \Theta \text{ is regular}\\
f_{\theta} = f_{x,} f_{y} \text{ , } \xi_{\theta} = \sqrt{f_{x} f_{y}}\\
\xi'_{\theta} = \xi'_{x,\theta}\xi_{y,\theta} + \xi_{x,\theta}\xi'_{y,\theta} \text{ , } I(\theta)=I_x(\theta)+I_y(\theta) 
\end{array}\right.
\]
Then if {$ f_{\theta}(x) ,I_1(\theta) $} is regular, the sample model $ \bigotimes^n f_{\theta}(x_i)$ is also regular and $I_n(\theta) = n I_1(\theta)$

\section{Efficiency of regular models}
\paragraph{Informations inequality}
\[
\text{if }
\left\{
\begin{array}{l}
f_{\theta}, \theta \in \Theta \text{ regular model} \\
\theta \mapsto \mathbb{E}_{\theta}[T^2(X)] \text{ locally bounded} \\
T(X) \text{ estimator of } g(\theta) \text{ , } I(\theta) > 0 \\
T(X) \text{ is estimator's biais}
\end{array}\right.
\text{ then }
\mathbb{E}_{\theta}[(T(X)-g(\theta))^2] \geq 
b^2(\theta) + \frac{[g'(\theta)+b'(\theta)]^2}{I(\theta)}
\]

\paragraph{Crame-Rao bound}
\[
\text{if }
\left\{
\begin{array}{l}
f_{\theta}, \theta \in \Theta \text{ regular model} \\
\widehat{\theta}_n(X) \text{ is unbiased estimator of } \theta \\
\textbf{Var}_{\theta}[\widehat{\theta}_n] \text{ locally bouned, } I(\theta) > 0 \\
\end{array}\right.
\text{ then  }
\hspace{5mm}
R(\widehat{\theta}_n(X),\theta) =
\mathbb{E}_{\theta}[(\widehat{\theta}_n(X)-\theta)^2] \geq 
\frac{1}{nI_1(\theta)}
\]
\paragraph{Asymtotical efficiency} A regular model ${f_{\theta}, I_1(\theta), \theta \in \Theta}$ where 
$\widehat{g}_n(X)$ is an estimator of $g(\theta)$ is said asymtotically efficient if
\[
\sqrt{n}(\widehat{g}_n(X) - g(\theta) )  \rightsquigarrow   \mathcal{N}(0,\sigma^{2}(\theta) ) 
\text{ with }
\hspace{10mm}
\left\{
\begin{array}{l}
\sigma^{2}(\theta) \leq \frac{g'^2(\theta)}{I_1(\theta)} \\
g'(\theta) \neq 0  \\
I_1(\theta) > 0 
\end{array}\right.
\]  



\section{Improving estimators}
\paragraph{Unbiazing an estimator}
\paragraph{Rao-Blackwell}
\paragraph{One step method} if the maximum likelihood is can not compute directly, one can use Newton method for finding numerically 
the solution of $L'_n = 0 $
\[
\widetilde{\theta}_n =  T_n - \frac{L'_n(T_n)}{L''_n(T_n)} 
\hspace{5mm}
L'_n(T_n) \sim \frac{L_n(T_n + c\sqrt{n}) - L_n(T_n - c\sqrt{n}) }{2c\sqrt{n}}
\hspace{5mm}
L''_n(T_n) \sim -nI_1(T_n)
\]
we have the one step modified estimator
\[
\widetilde{\theta}_n =  T_n + \frac{L_n(T_n + c\sqrt{n}) - L_n(T_n - c\sqrt{n}) }{2c\sqrt{n} I_1(T_n)}
\]
If the model is regular and $\sqrt{n}(T_n - \theta)$ is bouned in probability (if convergent in distribution), then the modified estimator above is asymtotically efficient


\section{Bayesian formules}
\[
\left\{
\begin{array}{ll}
h(x,\theta) = f(x|\theta)g(\theta) = g(\theta|x)\overline{f}(x)  & \text{is the joint density}  \\
g(\theta)   &\text{a given prior law }                                         \\ 
g(\theta|x) &\text{the posterior law }                            
\end{array}\right.
\]
Then we can compute the marginal distribution of $x$ and the posterior distribution 
\[
\overline{f}(x) = \int_{\Theta} f(x|\theta)g(\theta) d\theta
\hspace{2cm}
g(\theta|x) = \frac{ f(x|\theta)g(\theta) }{ \overline{f}(x) }
\]
Giving the loss function $l(\theta,\textbf{T(X)})$, the risks are computed by :
\[
\left\{
\begin{array}{ll}
R_{\theta}(\theta, \textbf{T(X)}) = \int_{\Omega} l(\theta,\textbf{T(X)}) d\mathbb{P}_{\theta}(X)  & \text{usual risk}  \\
R_{\textbf{B}}(\theta, \textbf{T(X)}) =   \int_{\Theta} R_{\theta}(\theta, \textbf{T(X)}) d\mathbb{P}(\theta) = 
\int_{\Theta} d\mathbb{P}(\theta) \int_{\Omega} l(\theta,\textbf{T(X)}) d\mathbb{P}_{\theta}(X)
& \text{Baysian risk} 
\end{array}\right.
\]
If all mesures are dominated by Lebesgues, the baysian risk is 
\[
R_{\textbf{B}}(\theta, \textbf{T(X)}) = \int_{\Theta} g(\theta) d\theta \int_{\Omega} l(\theta,\textbf{T}(x))   f(x|\theta) dx  
= \int_{\Omega}  \overline{f}(x) dx\int_{\Theta} l(\theta,\textbf{T}(x))  g(\theta|x) d\theta 
\]
The Baysian estimator is the one who minimize the Baysian risk, or equivalently 
\[
\widehat{\theta}^{B} = \text{argmin}_{\textbf{T} } \int_{\Theta} l(\theta,\textbf{T}(x))  g(\theta|x) d\theta 
\]
\end{document}